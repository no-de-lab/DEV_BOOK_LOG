검색 엔진 인덱싱, 웹 아카이빙, 웹 마이닝과 웹 모니터링으로 쓰이는 크롤러는 처리해야할 규모에 따라 복잡도가 달라진다.

## 개략적 설계
1. 시작 URL 집합: 크롤링을 시작하는 출발점이다. 여러 전략에 따라 달라질 수 있다. 시작 URL들을 미수집 URL 저장소에 저장한다.
2. HTML 다운로더는 이 저장소에서 목록을 가져온다. -> Robots.txt의 규칙을 확인해야한다. 
3. 도메인 이름 변환기를 사용, IP 주소를 알아내고 해당 IP주소로 접속해 웹 페이지를 다운받는다. 
4. 컨텐츠 파서는 다운된 html 페이지를 파싱하여 올바른 형식을 갖춘 페이지인지 검증한다. 
5. 컨텐츠 파싱과 검증이 끝나면 중복 컨텐츠인지 확인한다. 이미 시스템에 저장되어있는 컨텐츠임을 알아내야한다. 웹페이지의 해시값을 비교한다.
6. 중복 컨텐츠인지 확인하기위해 해당 페이지가 이미 저장소에 있는지를 확인한다. 
7. URL 추출기는 해당 HTML에서 링크를 골라낸다. 
8. 골라낸 링크를 URL 필터로 전달한다. 
9. 필터링이 끝나고 남은 url만 중복 주소 판별 단계로 전달한다. 
10. 이미 처리한 url인지 확인하기위해, 저장소에 보관된 url인지 살핀다. 
11. 저장소에 없는 url은 url 저장소에 저장할 뿐 아니라 미수집 url 저장소에도 전달한다. 

## 크롤링 프로세스: 그래프 탐지
그래프 크기가 어느정도로 깊숙한지 몰라서 DFS보다는 BFS를 사용한다. 이는 그러나 url 간에 우선순위를 두지않아 뭐가 중요한지를 파악할 수가 없고, 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아가므로 서버는 수많은 요청으로 과부하에 걸리게 된다. 이를 예의 없는 크롤러라 부르기도 한다. 

이를 미수집 url 저장소로 쉽게 해결 가능하다. 이 저장소를 잘 구현하면 url 사이의 우선순위와 신선도를 구별하는 크롤러를 구현할 수 있다. 

- 동일한 웹사이트에 대해서는 한번에 한페이지만 요청한다. 
    - 웹사이트의 호스트명과 다운로드를 수행하는 작업 스레드 사이의 관계를 유지한다. 작업 스레드는 전달된 url을 다운로드하는 작업을 수행하는데, 작업들 사이에는 일정한 지연시간을 둘 수 있다.
    - 같은 호스트에 속한 url은 언제나 같은 큐로 간다. 큐 선택기는 큐들을 순회하면서 큐에서 url을 꺼내 해당 큐에서 나온 주소를 다운로드하도록 작업스레드에 전달한다.  
- 우선순위: URL을 입력으로 받아 우선순위를 계산하고 그 순위별로 큐가 하나씩 할당된다. 우선순위가 높으면 선택될 확률도 높아진다. 
- 2개의 큐를 두어, 전면 큐에는 우선순위 결정과정을 처리하게 하고 후면에는 크롤러가 예의바르게 행동하도록 보증한다. 

## HTML 다운로더의 성능 최적화
1. 분산 크롤링
2. 도메인 이름 변환 결과 캐시
3. 지역성: 지역별로 크롤링 작업 수행하는 서버를 분산한다. 
4. 짧은 타임아웃: 최대 얼마나 기다릴지를 미리 정한다. 

### HTML 다운로더의 안정성
1. 안정해시 이용
2. 크롤링 상태 및 수집 데이터 저장
3. 예외 처리
4. 데이터 검증

### HTML 다운로더의 확장성
- 중복 컨텐츠를 확인한 후, URL추출기 이외에 사진 다운로더나 웹 모니터 등을 추가해볼 수 있어야한다.
### HTML 다운로더의 문제있는 컨텐츠 감지 및 회피
1. 중복 컨텐츠: 해시나 체크섬으로 파악하여 피한다.
2. 거미 덫: 크롤러를 무한 루프에 빠뜨리도록 설계하였다. url의 최대 길이를 제한하면 회피할 수 있다. 하지만 만능 해결책은 없다. 
3. 데이터 노이즈 제외

